{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- comment this is a comment heading -->\n",
    "# CS 229: Machine Learning\n",
    "\n",
    "<!-- How to make a checkbox: <input type=\"checkbox\">Completed?</input> -->\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "- **Labelled Data**: Data with known labels (e.g., images, text)\n",
    "- **Training Set**: A subset of the labelled data used to train a model\n",
    "- **Testing Set**: A subset of the labelled data used to evaluate the trained model\n",
    "\n",
    "The hypothesis function $h(x)$ is a function that maps input features to output labels. We can use supervised learning algorithms to learn this hypothesis function from the training set. If we have two features, we can write the hypothesis function as follows:\n",
    "\n",
    "$ h(x) = \\sum_{j=0}^{2} \\theta_j x_j $\n",
    "\n",
    "where $\\theta_j$ are the weights and $x_j$ are the input features; we define $x_0 = 1$ and therefore $\\theta_0$ is the bias term.\n",
    "\n",
    "$ \\theta = \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\end{pmatrix} $\n",
    "$ x = \\begin{pmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{pmatrix} $\n",
    "\n",
    "This is an affine function (which means a linear function with a constant term)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Terminology |\n",
    "| --- |\n",
    "| $ \\theta = $ \"parameters\" |\n",
    "| $ m = $ \"number of training examples\" |\n",
    "| $ n = $ \"number of features\" |\n",
    "| $ x = $ \"input data / features\" |\n",
    "| $ y = $ \"output labels / target data\" |\n",
    "| $ (x, y) $ = \"training example\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=(10, 3) theta.shape=(3,)\n",
      "[3.41294576 1.75326503 4.24482777 5.43309214 2.44961306 2.74346739\n",
      " 3.06829328 3.92873228 3.60080464 4.67245747]\n",
      "3.5307498829082817\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(42)\n",
    "\n",
    "# Define the hypothesis function\n",
    "def h(x, theta):\n",
    "    return x @ theta\n",
    "    # return np.dot(x, theta)\n",
    "\n",
    "n = 2 # number of features (excluding bias term)\n",
    "m = 10 # number of training examples\n",
    "\n",
    "features = n+1 # including bias term\n",
    "\n",
    "x = np.random.rand(m, features) # input data\n",
    "theta = np.random.rand(features)\n",
    "# x = np.array([[1,2,3], [4,5,6]])\n",
    "\n",
    "theta = np.array([1, 2, 3])\n",
    "print(f'{x.shape=} {theta.shape=}')\n",
    "# print(f'{x=} {theta=}')\n",
    "print(h(x, theta))\n",
    "print(np.mean(h(x, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to choose the parameters $\\theta$ that minimize the error between the predicted output and the actual target data.\n",
    "\n",
    "Choose $ \\theta $ such that $ h(x) \\approx y $.\n",
    "\n",
    "$ h_\\theta(x) = h(x) $\n",
    "\n",
    "$ x_{}^{(i)} $ is the $i$-th training example.\n",
    "\n",
    "Cost function: $ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (h_\\theta(x_{}^{(i)}) - y_{}^{(i)})^2 $\n",
    "\n",
    "minimize $ J(\\theta) $ to find the optimal $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Start with some initial guess for $\\theta$, and iteratively update it based on the gradient of the cost function. The gradient is a vector that points in the direction of steepest ascent of the cost function. The learning rate determines how fast the parameters are updated.\n",
    "\n",
    "The formula for updating $\\theta$ using gradient descent is:\n",
    "\n",
    "$ \\theta_j := \\theta_j - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$ for all $j$.\n",
    "\n",
    "Where:\n",
    "\n",
    "- $j$ is the index of the parameter being updated\n",
    "- $J(\\theta)$ is the cost function evaluated at $\\theta$\n",
    "- $\\alpha$ is the learning rate\n",
    "\n",
    "The learning rate should be chosen carefully, as too small a learning rate can cause slow convergence, while too large a learning rate can cause overshooting and instability.\n",
    "\n",
    "> \"If the features are scaled to +1 to -1, learning rate could be chosen to start at 0.01.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=(4, 3) y.shape=(4,)\n",
      "0: theta=array([0.91531332, 1.13420377, 3.55656283]) grad=array([ -22.89625677,  -91.58502706, -366.34010824])\n",
      "10000: theta=array([1.9903157 , 1.00826966, 2.99856876]) grad=array([-0.0013549 , -0.00541959, -0.02167835])\n",
      "20000: theta=array([1.99984949, 1.00012853, 2.99997776]) grad=array([-2.10577190e-05, -8.42308761e-05, -3.36923504e-04])\n",
      "30000: theta=array([1.99999766, 1.000002  , 2.99999965]) grad=array([-3.27277760e-07, -1.30911104e-06, -5.23644417e-06])\n",
      "40000: theta=array([1.99999996, 1.00000003, 2.99999999]) grad=array([-5.08653386e-09, -2.03461354e-08, -8.13845418e-08])\n",
      "50000: theta=array([2., 1., 3.]) grad=array([-7.90620902e-11, -3.16248361e-10, -1.26499344e-09])\n",
      "60000: theta=array([2., 1., 3.]) grad=array([-1.22213351e-12, -4.88853402e-12, -1.95541361e-11])\n",
      "70000: theta=array([2., 1., 3.]) grad=array([-5.68434189e-14, -2.27373675e-13, -9.09494702e-13])\n",
      "80000: theta=array([2., 1., 3.]) grad=array([-5.68434189e-14, -2.27373675e-13, -9.09494702e-13])\n",
      "90000: theta=array([2., 1., 3.]) grad=array([-5.68434189e-14, -2.27373675e-13, -9.09494702e-13])\n",
      "Final theta: [2. 1. 3.]\n",
      "Loss after training: 2.3534087385458445e-26\n",
      "[ 6. 16. 32. 54.]\n",
      "311.99999999999704\n",
      "np.linalg.inv(X.T @ X) @ (X.T @ y)=array([2., 1., 3.])\n",
      "(np.linalg.inv(X.T @ X) @ (y.T @ X)).T=array([2., 1., 3.])\n"
     ]
    }
   ],
   "source": [
    "def update_theta(theta, alpha, grad):\n",
    "    return theta - alpha * grad\n",
    "\n",
    "def gradient_descent(X, y, alpha=0.001, num_iterations=100000):\n",
    "    \"\"\"\n",
    "    (Batch) gradient descent algorithm for linear regression.\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    theta = np.random.rand(n)\n",
    "    # theta = np.zeros(n)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Calculate the predictions\n",
    "        y_pred = X @ theta\n",
    "        # Calculate the gradients\n",
    "        grad = ((y_pred - y) @ X)\n",
    "        # Update the parameters\n",
    "        theta = update_theta(theta, alpha, grad)\n",
    "        # print(f'{i}: {theta=} {grad=}')\n",
    "    return theta\n",
    "\n",
    "def stochastic_gradient_descent(X, y, alpha=0.005, num_iterations=100000):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent algorithm for linear regression.\n",
    "    This version uses a single training example at each iteration compared to the (batch) gradient descent\n",
    "    above which updates using the entire dataset at each iteration. This can be faster than the batch\n",
    "    gradient descent but may not converge as quickly.\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    theta = np.random.rand(n)\n",
    "    # theta = np.zeros(n)\n",
    "    for i in range(num_iterations):\n",
    "        for j in range(X.shape[0]):\n",
    "            y_pred_j = X[j] @ theta\n",
    "            error = y_pred_j - y[j]\n",
    "            grad = error * X[j]\n",
    "            theta = update_theta(theta, alpha, grad)\n",
    "        if i % 10000 == 0:\n",
    "            print(f'{i}: {theta=} {grad=}')\n",
    "    return theta\n",
    "\n",
    "# Input:\n",
    "#X = np.array([[1, 1, 1], [1, 2, 4], [1, 3, 9], [1, 4, 16]]) # first feature is the bias term, the x, then x^2\n",
    "\n",
    "# define a function to translate x into a vector of 1,x,x^2\n",
    "X_raw = [1, 2, 3, 4]\n",
    "def translate_x(x):\n",
    "    return np.array([1, x, x**2])\n",
    "\n",
    "X = np.array([translate_x(x) for x in X_raw])\n",
    "\n",
    "# Output:\n",
    "# y = np.array([1, 4, 9, 16])  # non-linear -- y = x^2 \n",
    "y = np.array([6, 16, 32, 54])  # non-linear -- y = 3x^2 + x + 2\n",
    "# y = np.array([7, 12, 17, 22])  # y = 5x + 2\n",
    "\n",
    "print(f'{X.shape=} {y.shape=}')\n",
    "#theta = gradient_descent(X, y)\n",
    "theta = stochastic_gradient_descent(X, y)\n",
    "print(\"Final theta:\", theta)\n",
    "\n",
    "def calculate_loss(X, y, theta):\n",
    "    predictions = X @ theta\n",
    "    loss = np.sum((predictions - y) ** 2)\n",
    "    return loss\n",
    "\n",
    "print(f'Loss after training: {calculate_loss(X, y, theta)}')\n",
    "\n",
    "print(f'{X @ theta}')\n",
    "print(f'{translate_x(10) @ theta}')\n",
    "print(f'{np.linalg.inv(X.T @ X) @ (X.T @ y)=}') # Algebraic solution when derivative of J(theta) = 0\n",
    "print(f'{(np.linalg.inv(X.T @ X) @ (y.T @ X)).T=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochachastic Gradient Descent\n",
    "\n",
    "When the number of training examples is large, it can be too expensive to analyse the entire dataset for each iteration. If for example your dataset is 1TB, just reading that dataset from disk becomes prohibitively slow.\n",
    "\n",
    "Using stochastic gradient descent (SGD) allows you to perform an update per training example, rather than going through them all. This makes it much faster and more scalable for large datasets.\n",
    "\n",
    "If you imagine gradient descent as a hill-climbing algorithm, GD is like taking one step at a time down the slope of the loss function. It starts at the top of the hill (the initial theta values) and moves downhill until it reaches the bottom (the minimum loss). GD chooses the steepest path to move down the hill at each step, whereas SGD only looks at a single example at a time and uses only that knowledge to step down the slope -- this may not be the optimium path and instead may move around noisily towards the minimum.\n",
    "\n",
    "SGD by virtue of looking at individual examples, may also not converge exactly on the minimum and instead oscillate around the mininum. In contrast, GD will always converge to the global minimum if it is convex (i.e., there are no local minima). SGD however, does not guarantee convergence.\n",
    "\n",
    "The learning rate corresponds roughly to how big a step you take down the slope at each iteration. If you set the learning rate too high, your model may overshoot the minimum and diverge. If you set it too low, your model will converge slowly and might never reach the minimum. The optimal learning rate depends on the specific problem and dataset.\n",
    "\n",
    "You can also adjust the learning rate as e.g. 1 / (t + 1) where t is the iteration number. This is called a decay schedule.\n",
    "\n",
    "# Mini-batch SGD\n",
    "\n",
    "In mini-batch SGD, you randomly sample a subset of the data at each iteration. This can help reduce variance and improve convergence speed compared to vanilla SGD.\n",
    "\n",
    "Randomly sampling for each batch is necessary for example if you consider that a dataset may be sorted such that all positive examples come before negative ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 0.23399999999999993\n",
      "Iteration 101: Loss = 0.051146342994597964\n",
      "Iteration 201: Loss = 0.02311130942281546\n",
      "Iteration 301: Loss = 0.010443222173157794\n",
      "Iteration 401: Loss = 0.004718940297267173\n",
      "Iteration 501: Loss = 0.0021323301525087244\n",
      "Iteration 601: Loss = 0.0009635281637131656\n",
      "Iteration 701: Loss = 0.00043538591862812734\n",
      "Iteration 801: Loss = 0.0001967362297009953\n",
      "Iteration 901: Loss = 8.889847471163438e-05\n",
      "Updated weights: [-0.9800318   0.49425807]\n"
     ]
    }
   ],
   "source": [
    "def sgd(X, y, lr=0.01, weights=None):\n",
    "    \"\"\"\n",
    "    Perform stochastic gradient descent on the given data.\n",
    "    Args:\n",
    "        X (numpy.ndarray): Input features.\n",
    "        y (numpy.ndarray): Target labels.\n",
    "        lr (float): Learning rate.\n",
    "    Returns:\n",
    "        numpy.ndarray: Updated weights after SGD.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.zeros(X.shape[1])\n",
    "    for i in range(len(y)):\n",
    "        gradient = 2 * X[i] * (np.dot(X[i], weights) - y[i])\n",
    "        weights -= lr * gradient\n",
    "    return weights\n",
    "\n",
    "X = np.array([[1, 2], [1, 4]])\n",
    "y = np.array([0, 1])\n",
    "weights = None\n",
    "for i in range(1000):\n",
    "    weights = sgd(X, y, weights=weights)  # This will update the weights based on the current data and learning rate.\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i+1}: Loss = {np.mean((y - np.dot(X, weights))**2)}\")\n",
    "\n",
    "print(\"Updated weights:\", weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Derivatives\n",
    "\n",
    "\n",
    "$ \\nabla_A trAB = B^T $\n",
    "\n",
    "> $ AB = \\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{pmatrix} \\begin{pmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\end{pmatrix} = \\begin{pmatrix} A_{11}B_{11} + A_{12}B_{21} & A_{11}B_{12} + A_{12}B_{22}  \\\\ A_{21}B_{11} + A_{22}B_{21} & A_{21}B_{12} + A_{22}B_{22}  \\end{pmatrix} $\n",
    ">\n",
    "> $ tr(AB) = A_{11}B_{11} + A_{12}B_{21} + A_{21}B_{12} + A_{22}B_{22} $\n",
    ">\n",
    "> $ tr(AB) = \\sum_{i,j} A_{ij} B_{ji} $\n",
    ">\n",
    "> Elementwise differentiation of the trace:\n",
    "> $ \\frac{\\partial}{\\partial A_{ij}} tr(AB) = B_{ji} $\n",
    "> \n",
    "> Given that:\n",
    "> $ \\nabla_A f(A) = \\begin{pmatrix} \\frac{\\partial}{\\partial A_{11}} f(A) & \\frac{\\partial}{\\partial A_{12}} f(A) \\\\ \\frac{\\partial}{\\partial A_{21}} f(A) & \\frac{\\partial}{\\partial A_{22}} f(A) \\end{pmatrix} $\n",
    ">\n",
    "> => $ \\nabla_A tr(AB) = \\begin{pmatrix} \\frac{\\partial}{\\partial A_{11}} tr(AB) & \\frac{\\partial}{\\partial A_{12}} tr(AB) \\\\ \\frac{\\partial}{\\partial A_{21}} tr(AB) & \\frac{\\partial}{\\partial A_{22}} tr(AB) \\end{pmatrix} = \\begin{pmatrix} B_{11} & B_{21} \\\\ B_{12} & B_{22} \\end{pmatrix} = B^T $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate a regression problem\n",
    "X, y = make_regression(\n",
    "    n_samples=100,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    noise = 10,\n",
    "    random_state=25\n",
    "    )\n",
    "\n",
    "# Visualize feature at index 1 vs target\n",
    "plt.subplots(figsize=(8, 5))\n",
    "plt.scatter(X[:, 0], y, marker='o')\n",
    "plt.scatter(X[:, 1], y, marker='o')\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytical solution for linear regression\n",
    "\n",
    "The analytical solution to the linear regression problem can be derived using matrix algebra. The goal is to find the coefficients \\(\\theta\\) that minimize the mean squared error (MSE) between the predicted and actual target values.\n",
    "\n",
    "The cost function for linear regression from earlier is given by:\n",
    "\n",
    "$ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (h_\\theta(x_{}^{(i)}) - y_{}^{(i)})^2 $\n",
    "\n",
    "\n",
    "## Matrix definition of cost function\n",
    "If we define $X$ as a matrix whose rows correspond to the feature vectors $x^{(i)}$ and $\\hat{y}$ as a vector of target values, the cost function becomes:\n",
    "$$ J(\\theta) = \\frac{1}{2} (X \\theta - y)^T (X \\theta - y) $$\n",
    "\n",
    "We can see this as follows:\n",
    "\n",
    "$X$ is an $m \\times n$ matrix, where $m$ is the number of samples and $n$ is the number of features. The vector $\\hat{y}$ is a vector of length $m$, containing the target values for each sample.\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "x_{0}^{(1)} & x_{1}^{(1)} & \\cdots & x_{n}^{(1)} \\\\\n",
    "x_{0}^{(2)} & x_{1}^{(2)} & \\cdots & x_{n}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "x_{0}^{(m)} & x_{1}^{(m)} & \\cdots & x_{n}^{(m)} \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$ \\hat{y} = \\begin{bmatrix}\n",
    "y_{}^{(1)} \\\\\n",
    "y_{}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "y_{}^{(m)} \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "With $h_\\theta(x^{(i)}) = (x^{(i)})^T \\theta $, we can write the error vector as:\n",
    "\n",
    "$$ X\\theta - y = \\begin{bmatrix}\n",
    "(x^{(1)})^T \\theta - y_{}^{(1)} \\\\\n",
    "(x^{(2)})^T \\theta - y_{}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "(x^{(m)})^T \\theta - y_{}^{(m)} \\\\\n",
    "\\end{bmatrix} =  \\begin{bmatrix}\n",
    "h_\\theta(x^{(1)}) - y_{}^{(1)} \\\\\n",
    "h_\\theta(x^{(2)}) - y_{}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "h_\\theta(x^{(m)}) - y_{}^{(m)} \\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Using $z^Tz = \\sum_{j} z_j^2 $, with $z = X\\theta - y$ and $z_j = (h_\\theta(x^{(i)}) - y_{}^{(i)})$, we get:\n",
    "\n",
    "$$ \\frac{1}{2} (X \\theta - y)^T (X \\theta - y) = \\frac{1}{2} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 = J(\\theta) $$\n",
    "\n",
    "\n",
    "##Â Minimizing the Cost Function\n",
    "\n",
    "To minimize this cost function, we take the derivative with respect to $\\theta$ and set it to zero:\n",
    "$ \\nabla_\\theta J(\\theta) = 0 $\n",
    "\n",
    "And using the following rules:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "\\frac{d}{dx} (Ax) &= A \\\\\n",
    "\\frac{d}{dx} (x^TA) &= A^T \\\\\n",
    "\\frac{d}{dx} (x^T Ax) &= 2x^T A   - \\text{(if A is symmetric)} \\\\\n",
    "\\end{aligned} $$\n",
    "\n",
    "$ X^TX $ is symmetric.\n",
    "\n",
    "And finally, we get:\n",
    "\n",
    "$ \\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta) &= \\nabla_\\theta \\frac{1}{2} (X \\theta - y)^T (X \\theta - y) \\\\\n",
    "                        &= \\frac{1}{2} \\nabla_\\theta ( (X\\theta)^T (X\\theta) - (X\\theta)^T y - y^T (X \\theta) - y^T y) \\\\\n",
    "                        &= \\frac{1}{2} \\nabla_\\theta ( \\theta^T X^T X \\theta - \\theta^T X^T y - y^T X \\theta - y^T y ) \\\\\n",
    "                        &= \\frac{1}{2} ( 2 \\theta^T (X^T X) - (X^T y)^T - (y^T X) ) \\\\\n",
    "                        &= \\frac{1}{2} ( 2 \\theta^T X^T X - y^TX - y^T X ) \\\\\n",
    "                        &= \\frac{1}{2} ( 2 \\theta^T X^T X - 2 y^TX ) \\\\\n",
    "                        &= \\theta^T X^T X - y^TX \\\\\n",
    "\\end{aligned} $\n",
    "\n",
    "Setting this to 0 finds us the points where it is minimised:\n",
    "\n",
    "$ \\theta^T X^T X - y^TX = 0 $\n",
    "\n",
    "This leads to the normal equations:\n",
    "\n",
    "$ \\theta^T X^T X = y^TX $\n",
    "\n",
    "$ (\\theta^T X^T X)^T = (y^TX)^T $\n",
    "\n",
    "$ X^T X \\theta = X^T y $\n",
    "\n",
    "These equations can be solved for $\\theta$:\n",
    "\n",
    "$$ \\theta = (X^T X)^{-1} X^T y $$\n",
    "\n",
    "In Python using NumPy, this looks like\n",
    "\n",
    "```\n",
    "    theta = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "    theta = (np.linalg.inv(X.T @ X) @ (y.T @ X)).T\n",
    "```\n",
    "\n",
    "\n",
    "(This was with a bunch of help from especially these two sources:\n",
    "\n",
    "https://math.stackexchange.com/questions/4177039/deriving-the-normal-equation-for-linear-regression\n",
    "\n",
    "https://en.wikipedia.org/wiki/Matrix_calculus#Vector-by-vector_identities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally Weighted Linear Regression\n",
    "\n",
    "Local weighted linear regression is a method for fitting a linear model to data points that are close to each other. It uses a weighting function to give more weight to the data points that are closer to the target point.\n",
    "\n",
    "The weight function looks like a Guassian but is not exactly a Guassian. It is defined as:\n",
    "$$ w_i = \\exp(-\\frac{(x^{(i)} - x)^2}{2\\tau^2})  $$\n",
    "\n",
    "The formula for the local weighted linear regression is:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m w_i (h_\\theta(x_{}^{(i)}) - y_{}^{(i)})^2 $$\n",
    "\n",
    "In matrix form:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2} (W X \\theta - y)^T (W X \\theta - y) $$\n",
    "\n",
    "where W is a diagonal matrix with the weights $w_i$ on the diagonal.\n",
    "\n",
    "Going through the derivative as before:\n",
    "\n",
    "$ \\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta) &= \\nabla_\\theta \\frac{1}{2} (WX \\theta - y)^T (WX \\theta - y) \\\\\n",
    "                        &= \\frac{1}{2} \\nabla_\\theta ( (WX\\theta)^T (WX\\theta) - (WX\\theta)^T y - y^T (WX \\theta) - y^T y) \\\\\n",
    "                        &= \\frac{1}{2} \\nabla_\\theta ( \\theta^T X^T W^T X \\theta - \\theta^T X^T W^T y - y^T WX \\theta - y^T y ) \\\\\n",
    "                        &= \\frac{1}{2} ( 2 \\theta^T (X^T W^T X) - (X^T W^T y)^T - (y^T WX) ) \\\\\n",
    "                        &= \\frac{1}{2} ( 2 \\theta^T X^T W^T X - y^TWX - y^T WX ) \\\\\n",
    "                        &= \\frac{1}{2} ( 2 \\theta^T X^T W^T X - 2 y^TWX ) \\\\\n",
    "                        &= \\theta^T X^T W^T X - y^TWX \\\\\n",
    "\\end{aligned} $\n",
    "\n",
    "Set to 0 and use the fact W is symmetric, gets to:\n",
    "\n",
    "$ \\begin{aligned}\n",
    "\\theta^T X^T W^T X - y^TWX &= 0 \\\\ \n",
    "\\theta^T X^T W^T X &= y^TWX \\\\\n",
    "(\\theta^T X^T W^T X)^T &= (y^TWX)^T \\\\\n",
    "X^T WX \\theta &= X^T W^T y \\\\\n",
    "\\theta &= (X^T WX)^{-1} (X^T W^T y) \\\\\n",
    "\\theta &= (X^T WX)^{-1} (X^T W y) \\\\\n",
    "\\end{aligned} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W=array([[3.72007598e-044, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000],\n",
      "       [0.00000000e+000, 1.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000],\n",
      "       [0.00000000e+000, 0.00000000e+000, 3.72007598e-044,\n",
      "        0.00000000e+000],\n",
      "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        1.91516960e-174]])\n",
      "theta=array([8.])\n",
      "16.0\n",
      "W=array([[1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.]])\n",
      "theta=array([11.66666667])\n",
      "23.333333332648888\n",
      "gradient_descent(X, y)=array([11.66666667])\n"
     ]
    }
   ],
   "source": [
    "def local_weighted_linear_regression(X, y, tau, target_x):\n",
    "    # Calculate the weights for each data point\n",
    "    W = np.exp(-((X - target_x) ** 2) / (tau ** 2))\n",
    "    W = np.diag(W.squeeze())\n",
    "    print(f'{W=}')\n",
    "    theta = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ y)\n",
    "    print(f'{theta=}')\n",
    "    return [target_x] @ theta\n",
    "\n",
    "X_raw = [1, 2, 3, 4]\n",
    "def translate_x(x):\n",
    "    return np.array([x])\n",
    "\n",
    "X = np.array([translate_x(x) for x in X_raw])\n",
    "# y = np.array([1, 4, 9, 16])  # non-linear -- y = x^2 \n",
    "y = np.array([6, 16, 32, 54])  # non-linear -- y = 3x^2 + x + 2\n",
    "# y = np.array([7, 12, 17, 22])  # y = 5x + 2\n",
    "\n",
    "# Tau is the \"bandwidth\", or how far away from the point we're interested in we look for neighbors.\n",
    "# When very small, it only considers points very close to the target point.\n",
    "tau = 0.1\n",
    "print(local_weighted_linear_regression(X, y, tau, 2)) # close to 16, overfitting\n",
    "\n",
    "# Bigger values of tau spread out the weighting. When very large, effectively all points are weighted equally. \n",
    "tau = 100000\n",
    "print(local_weighted_linear_regression(X, y, tau, 2)) # close to the linear regression line, underfitting\n",
    "\n",
    "# This should be the same as the locally weighted result with large tau\n",
    "print(f'{gradient_descent(X, y)=}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
